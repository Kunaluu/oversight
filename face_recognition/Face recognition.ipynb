{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfba044",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888f4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ac83c",
   "metadata": {},
   "source": [
    "# Generating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfb87be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting samples is completed....\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset():\n",
    "    face_classifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    # face_classifier = cv2.CascadeClassifier(\"haarcascade_profileface.xml\")\n",
    "\n",
    "    def face_cropped(img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "        if len(faces) == 0:\n",
    "            return None\n",
    "        for (x, y, w, h) in faces:\n",
    "            cropped_face = img[y:y+h, x:x+w]\n",
    "        return cropped_face\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Change to 0 if 1 does not work\n",
    "    id = 1\n",
    "    img_id = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture image\")\n",
    "            continue\n",
    "        cropped_face = face_cropped(frame)\n",
    "        if cropped_face is not None:\n",
    "            img_id += 1\n",
    "            face = cv2.resize(cropped_face, (200, 200))\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            file_name_path = \"data_kunal/user.\" + str(id) + \".\" + str(img_id) + \".jpg\"\n",
    "            cv2.imwrite(file_name_path, face)\n",
    "            cv2.putText(face, str(img_id), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.imshow(\"Cropped face\", face)\n",
    "\n",
    "        if cv2.waitKey(1) == 13 or int(img_id) == 600:  # 13 is the ASCII character of Enter\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Collecting samples is completed....\")\n",
    "\n",
    "generate_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f1b055",
   "metadata": {},
   "source": [
    "# Training the classifier and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b9d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(data_dir):\n",
    "    path = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "     \n",
    "    faces = []\n",
    "    ids = []\n",
    "     \n",
    "    for image in path:\n",
    "        img = Image.open(image).convert('L')\n",
    "        imageNp = np.array(img, 'uint8')\n",
    "        id = int(os.path.split(image)[1].split(\".\")[1])\n",
    "         \n",
    "        faces.append(imageNp)\n",
    "        ids.append(id)\n",
    "         \n",
    "    ids = np.array(ids)\n",
    "     \n",
    "    # Train and save classifier\n",
    "    clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "    clf.train(faces,ids)\n",
    "    clf.write(\"classifier.xml\")\n",
    "train_classifier(\"data_kunal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3425ec",
   "metadata": {},
   "source": [
    "# Detecting the face and assigning labels :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97d5d9",
   "metadata": {},
   "source": [
    "# 1. In real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd006dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text, clf):\n",
    "#     gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "     \n",
    "#     for (x,y,w,h) in features:\n",
    "#         cv2.rectangle(img, (x,y), (x+w,y+h), color, 2 )\n",
    "         \n",
    "#         id, pred = clf.predict(gray_img[y:y+h,x:x+w])\n",
    "#         confidence = int(100*(1-pred/300))\n",
    "         \n",
    "#         if confidence>76:\n",
    "#             if id==1:\n",
    "#                 cv2.putText(img, \"Michelle\", (x,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "# #             if id==2:\n",
    "# #                 cv2.putText(img, \"Manish\", (x,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "#         else:\n",
    "#             cv2.putText(img, \"UNKNOWN\", (x,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 1, cv2.LINE_AA)\n",
    "     \n",
    "#     return img\n",
    " \n",
    "# # loading classifier\n",
    "# faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    " \n",
    "# clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "# clf.read(\"classifier.xml\")\n",
    " \n",
    "# video_capture = cv2.VideoCapture(0)\n",
    " \n",
    "# while True:\n",
    "#     ret, img = video_capture.read()\n",
    "#     img = draw_boundary(img, faceCascade, 1.3, 6, (255,255,255), \"Face\", clf)\n",
    "#     cv2.imshow(\"face detection\", img)\n",
    "     \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f0ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import datetime\n",
    "# import openpyxl\n",
    "\n",
    "# def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text, clf, session_id, label_confidence):\n",
    "#     gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    \n",
    "#     for (x, y, w, h) in features:\n",
    "#         cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "#         id, pred = clf.predict(gray_img[y:y + h, x:x + w])\n",
    "#         confidence = int(100 * (1 - pred / 300))\n",
    "        \n",
    "#         label = \"UNKNOWN\"\n",
    "#         if confidence > 75:\n",
    "#             if id == 1:\n",
    "#                 label = \"Michelle\"\n",
    "# #             if id == 2:\n",
    "# #                 label = \"Manish\"\n",
    "#         cv2.putText(img, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "#         label_confidence[label] = confidence\n",
    "    \n",
    "#     # Add session ID to the frame\n",
    "#     cv2.putText(img, f\"Session ID: {session_id}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "#     return img\n",
    "\n",
    "# def get_next_session_id(sheet):\n",
    "#     max_id = 0\n",
    "#     for row in sheet.iter_rows(min_row=2, max_col=1, values_only=True):\n",
    "#         if row[0] is not None:\n",
    "#             try:\n",
    "#                 session_id = int(row[0])\n",
    "#                 max_id = max(max_id, session_id)\n",
    "#             except ValueError:\n",
    "#                 continue\n",
    "#     return max_id + 1\n",
    "\n",
    "# def update_log(numeric_session_id, session_id, start_time, end_time, label_confidence):\n",
    "#     file_name = \"observation_logs.xlsx\"\n",
    "    \n",
    "#     # Load or create the workbook\n",
    "#     if os.path.exists(file_name):\n",
    "#         workbook = openpyxl.load_workbook(file_name)\n",
    "#     else:\n",
    "#         workbook = openpyxl.Workbook()\n",
    "#         workbook.active.append([\"ID\", \"Session ID\", \"Date\", \"Start Time\", \"End Time\", \"Duration (seconds)\", \"Label\", \"Confidence Level\"])\n",
    "\n",
    "#     sheet = workbook.active\n",
    "    \n",
    "#     # Calculate duration\n",
    "#     duration = (end_time - start_time).total_seconds()\n",
    "#     date = start_time.strftime(\"%Y-%m-%d\")\n",
    "#     start_str = start_time.strftime(\"%H:%M:%S\")\n",
    "#     end_str = end_time.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "#     # Append session data for each label\n",
    "#     for label, confidence in label_confidence.items():\n",
    "#         sheet.append([numeric_session_id, session_id, date, start_str, end_str, duration, label, f\"{confidence}%\"])\n",
    "    \n",
    "#     # Save the workbook\n",
    "#     workbook.save(file_name)\n",
    "\n",
    "# # Create 'recordings' directory if it doesn't exist\n",
    "# os.makedirs(\"recordings\", exist_ok=True)\n",
    "\n",
    "# # Create session ID based on the current timestamp\n",
    "# start_time = datetime.datetime.now()\n",
    "# session_id = start_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# # Create unique filename for recording\n",
    "# output_path = os.path.join(\"recordings\", f\"session_{session_id}.mp4\")\n",
    "\n",
    "# # Define codec and create VideoWriter object\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# fps = 20.0\n",
    "# frame_size = (640, 480)  # Match this to your webcam's resolution\n",
    "# out = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n",
    "\n",
    "# # Loading classifier\n",
    "# faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "# clf.read(\"classifier.xml\")\n",
    "\n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Dictionary to store label and confidence levels\n",
    "# label_confidence = {}\n",
    "\n",
    "# # Load or create the workbook and get the next session ID\n",
    "# file_name = \"observation_logs.xlsx\"\n",
    "# if os.path.exists(file_name):\n",
    "#     workbook = openpyxl.load_workbook(file_name)\n",
    "#     sheet = workbook.active\n",
    "#     numeric_session_id = get_next_session_id(sheet)\n",
    "# else:\n",
    "#     numeric_session_id = 1\n",
    "\n",
    "# while True:\n",
    "#     ret, raw_frame = video_capture.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Resize frame to match the VideoWriter frame size\n",
    "#     raw_frame = cv2.resize(raw_frame, frame_size)\n",
    "\n",
    "#     # Create a copy of the frame for processing\n",
    "#     display_frame = raw_frame.copy()\n",
    "\n",
    "#     # Process frame (detection and drawing)\n",
    "#     display_frame = draw_boundary(display_frame, faceCascade, 1.3, 6, (255, 255, 255), \"Face\", clf, session_id, label_confidence)\n",
    "\n",
    "#     # Write raw frame to video file (without detection)\n",
    "#     out.write(raw_frame)\n",
    "\n",
    "#     # Display the resulting frame with detection\n",
    "#     cv2.imshow(\"face detection\", display_frame)\n",
    "    \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Capture end time\n",
    "# end_time = datetime.datetime.now()\n",
    "\n",
    "# # Update the observation log\n",
    "# update_log(numeric_session_id, session_id, start_time, end_time, label_confidence)\n",
    "\n",
    "# # Release everything\n",
    "# video_capture.release()\n",
    "# out.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc41fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import datetime\n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, clf, session_id, label_confidence, frame_collection):\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    \n",
    "    for (x, y, w, h) in features:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        id, pred = clf.predict(gray_img[y:y + h, x:x + w])\n",
    "        confidence = int(100 * (1 - pred / 300))\n",
    "        \n",
    "        label = \"UNKNOWN\"\n",
    "        flag = \"red\"\n",
    "        if confidence > 76:\n",
    "            if id == 1:\n",
    "                label = \"Kunal\"\n",
    "                flag = \"green\"\n",
    "            # Add other known IDs here if necessary\n",
    "        \n",
    "        color = (0, 255, 0) if flag == \"green\" else (0, 0, 255)\n",
    "        cv2.putText(img, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "        label_confidence[label] = (confidence, flag)\n",
    "        \n",
    "        # Collect frames based on confidence\n",
    "        if label not in frame_collection:\n",
    "            frame_collection[label] = {\n",
    "                \"highest\": (confidence, img.copy()),\n",
    "                \"lowest\": (confidence, img.copy()),\n",
    "                \"all_confidences\": [(confidence, img.copy())]\n",
    "            }\n",
    "        else:\n",
    "            if confidence > frame_collection[label][\"highest\"][0]:\n",
    "                frame_collection[label][\"highest\"] = (confidence, img.copy())\n",
    "            if confidence < frame_collection[label][\"lowest\"][0]:\n",
    "                frame_collection[label][\"lowest\"] = (confidence, img.copy())\n",
    "            frame_collection[label][\"all_confidences\"].append((confidence, img.copy()))\n",
    "    \n",
    "    # Add session ID to the frame\n",
    "    cv2.putText(img, f\"Session ID: {session_id}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def get_next_session_id(sheet):\n",
    "    max_id = 0\n",
    "    for row in sheet.iter_rows(min_row=2, max_col=1, values_only=True):\n",
    "        if row[0] is not None:\n",
    "            try:\n",
    "                session_id = int(row[0])\n",
    "                max_id = max(max_id, session_id)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return max_id + 1\n",
    "\n",
    "def update_log(numeric_session_id, session_id, start_time, end_time, label_confidence):\n",
    "    file_name = \"observation_logs.xlsx\"\n",
    "    \n",
    "    # Load or create the workbook\n",
    "    if os.path.exists(file_name):\n",
    "        workbook = openpyxl.load_workbook(file_name)\n",
    "    else:\n",
    "        workbook = openpyxl.Workbook()\n",
    "        workbook.active.append([\"ID\", \"Session ID\", \"Date\", \"Start Time\", \"End Time\", \"Duration (seconds)\", \"Label\", \"Confidence Level\", \"Flag\"])\n",
    "\n",
    "    sheet = workbook.active\n",
    "    \n",
    "    # Calculate duration\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    date = start_time.strftime(\"%Y-%m-%d\")\n",
    "    start_str = start_time.strftime(\"%H:%M:%S\")\n",
    "    end_str = end_time.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    # Append session data for each label\n",
    "    for label, (confidence, flag) in label_confidence.items():\n",
    "        sheet.append([numeric_session_id, session_id, date, start_str, end_str, duration, label, f\"{confidence}%\", flag])\n",
    "    \n",
    "    # Save the workbook\n",
    "    workbook.save(file_name)\n",
    "\n",
    "def save_observation_frames(session_id, frame_collection):\n",
    "    os.makedirs(\"observations\", exist_ok=True)\n",
    "    \n",
    "    for label, frames in frame_collection.items():\n",
    "        highest_confidence_frame = frames[\"highest\"][1]\n",
    "        lowest_confidence_frame = frames[\"lowest\"][1]\n",
    "        \n",
    "        # Calculate average confidence frame\n",
    "        all_confidences = frames[\"all_confidences\"]\n",
    "        average_confidence_frame = all_confidences[len(all_confidences) // 2][1]\n",
    "        \n",
    "        highest_filename = os.path.join(\"observations\", f\"{session_id}_{label}_H.jpg\")\n",
    "        lowest_filename = os.path.join(\"observations\", f\"{session_id}_{label}_L.jpg\")\n",
    "        average_filename = os.path.join(\"observations\", f\"{session_id}_{label}_A.jpg\")\n",
    "        \n",
    "        cv2.imwrite(highest_filename, highest_confidence_frame)\n",
    "        cv2.imwrite(lowest_filename, lowest_confidence_frame)\n",
    "        cv2.imwrite(average_filename, average_confidence_frame)\n",
    "\n",
    "# Create 'recordings' directory if it doesn't exist\n",
    "os.makedirs(\"recordings\", exist_ok=True)\n",
    "\n",
    "# Create session ID based on the current timestamp\n",
    "start_time = datetime.datetime.now()\n",
    "session_id = start_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# Create unique filename for recording\n",
    "output_path = os.path.join(\"recordings\", f\"session_{session_id}.mp4\")\n",
    "\n",
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = 20.0\n",
    "frame_size = (640, 480)  # Match this to your webcam's resolution\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n",
    "\n",
    "# Loading classifier\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "clf.read(\"classifier.xml\")\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Dictionary to store label and confidence levels\n",
    "label_confidence = {}\n",
    "frame_collection = {}  # Dictionary to store frames based on confidence\n",
    "\n",
    "# Load or create the workbook and get the next session ID\n",
    "file_name = \"observation_logs.xlsx\"\n",
    "if os.path.exists(file_name):\n",
    "    workbook = openpyxl.load_workbook(file_name)\n",
    "    sheet = workbook.active\n",
    "    numeric_session_id = get_next_session_id(sheet)\n",
    "else:\n",
    "    numeric_session_id = 1\n",
    "\n",
    "while True:\n",
    "    ret, raw_frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame to match the VideoWriter frame size\n",
    "    raw_frame = cv2.resize(raw_frame, frame_size)\n",
    "\n",
    "    # Create a copy of the frame for processing\n",
    "    display_frame = raw_frame.copy()\n",
    "\n",
    "    # Process frame (detection and drawing)\n",
    "    display_frame = draw_boundary(display_frame, faceCascade, 1.3, 6, (255, 255, 255), clf, session_id, label_confidence, frame_collection)\n",
    "\n",
    "    # Write raw frame to video file (without detection)\n",
    "    out.write(raw_frame)\n",
    "\n",
    "    # Display the resulting frame with detection\n",
    "    cv2.imshow(\"face detection\", display_frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Capture end time\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Update the observation log\n",
    "update_log(numeric_session_id, session_id, start_time, end_time, label_confidence)\n",
    "\n",
    "# Save observation frames\n",
    "save_observation_frames(session_id, frame_collection)\n",
    "\n",
    "# Release everything\n",
    "video_capture.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a4cb3",
   "metadata": {},
   "source": [
    "# 2. On recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d1d1480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file selected. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# Function to draw boundary and recognize faces\n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text, clf):\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    \n",
    "    for (x, y, w, h) in features:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)\n",
    "        id, pred = clf.predict(gray_img[y:y+h, x:x+w])\n",
    "        confidence = int(100 * (1 - pred / 300))\n",
    "        \n",
    "        if confidence > 76:\n",
    "            if id == 1:\n",
    "                cv2.putText(img, \"Michelle\", (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(img, \"UNKNOWN\", (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Loading classifier\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "clf.read(\"classifier.xml\")\n",
    "\n",
    "# Function to select and process a video file\n",
    "def process_video():\n",
    "    # Set up file dialog to select a video\n",
    "    Tk().withdraw()  # Prevents an empty tkinter window from appearing\n",
    "    file_path = askopenfilename(initialdir=\"recordings\", title=\"Select a Recording\",\n",
    "                                filetypes=((\"MP4 files\", \"*.mp4\"), (\"AVI files\", \"*.avi\"), (\"All files\", \"*.*\")))\n",
    "    \n",
    "    if not file_path:\n",
    "        print(\"No file selected. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(file_path)\n",
    "    \n",
    "    while True:\n",
    "        ret, img = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        img = draw_boundary(img, faceCascade, 1.3, 6, (255, 255, 255), \"Face\", clf)\n",
    "        cv2.imshow(\"Face Detection\", img)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function to process video\n",
    "process_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e68ea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file selected. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "# Function to draw boundary and recognize faces\n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text, clf):\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "    \n",
    "    for (x, y, w, h) in features:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)\n",
    "        id, pred = clf.predict(gray_img[y:y+h, x:x+w])\n",
    "        confidence = int(100 * (1 - pred / 300))\n",
    "        \n",
    "        if confidence > 76:\n",
    "            if id == 1:\n",
    "                cv2.putText(img, \"Michelle\", (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(img, \"UNKNOWN\", (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Loading classifier\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "clf.read(\"classifier.xml\")\n",
    "\n",
    "# Function to select and process a video file\n",
    "def process_video():\n",
    "    # Set up DPI scaling\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Prevents the main tkinter window from appearing\n",
    "\n",
    "    # Increase DPI awareness\n",
    "    try:\n",
    "        from ctypes import windll\n",
    "        windll.shcore.SetProcessDpiAwareness(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set DPI awareness: {e}\")\n",
    "\n",
    "    initial_dir = os.path.join(os.getcwd(), \"recordings\")\n",
    "    file_path = filedialog.askopenfilename(initialdir=initial_dir, \n",
    "                                           title=\"Select a Recording for Face Detection\",\n",
    "                                           filetypes=((\"MP4 files\", \"*.mp4\"), (\"AVI files\", \"*.avi\"), (\"All files\", \"*.*\")))\n",
    "    \n",
    "    if not file_path:\n",
    "        print(\"No file selected. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(file_path)\n",
    "    \n",
    "    while True:\n",
    "        ret, img = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        img = draw_boundary(img, faceCascade, 1.3, 6, (255, 255, 255), \"Face\", clf)\n",
    "        cv2.imshow(\"Face Detection\", img)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function to process video\n",
    "process_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c5b35",
   "metadata": {},
   "source": [
    "<h1>Training using Dlib</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f09f55a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured face descriptor: 1\n",
      "Captured face descriptor: 2\n",
      "Captured face descriptor: 3\n",
      "Captured face descriptor: 4\n",
      "Captured face descriptor: 5\n",
      "Captured face descriptor: 6\n",
      "Captured face descriptor: 7\n",
      "Captured face descriptor: 8\n",
      "Captured face descriptor: 9\n",
      "Captured face descriptor: 10\n",
      "Captured face descriptor: 11\n",
      "Captured face descriptor: 12\n",
      "Captured face descriptor: 13\n",
      "Captured face descriptor: 14\n",
      "Captured face descriptor: 15\n",
      "Captured face descriptor: 16\n",
      "Captured face descriptor: 17\n",
      "Captured face descriptor: 18\n",
      "Captured face descriptor: 19\n",
      "Captured face descriptor: 20\n",
      "Captured face descriptor: 21\n",
      "Captured face descriptor: 22\n",
      "Captured face descriptor: 23\n",
      "Captured face descriptor: 24\n",
      "Captured face descriptor: 25\n",
      "Captured face descriptor: 26\n",
      "Captured face descriptor: 27\n",
      "Captured face descriptor: 28\n",
      "Captured face descriptor: 29\n",
      "Captured face descriptor: 30\n",
      "Captured face descriptor: 31\n",
      "Captured face descriptor: 32\n",
      "Captured face descriptor: 33\n",
      "Captured face descriptor: 34\n",
      "Captured face descriptor: 35\n",
      "Captured face descriptor: 36\n",
      "Captured face descriptor: 37\n",
      "Captured face descriptor: 38\n",
      "Captured face descriptor: 39\n",
      "Captured face descriptor: 40\n",
      "Captured face descriptor: 41\n",
      "Captured face descriptor: 42\n",
      "Captured face descriptor: 43\n",
      "Captured face descriptor: 44\n",
      "Captured face descriptor: 45\n",
      "Captured face descriptor: 46\n",
      "Captured face descriptor: 47\n",
      "Captured face descriptor: 48\n",
      "Captured face descriptor: 49\n",
      "Captured face descriptor: 50\n",
      "Captured face descriptor: 51\n",
      "Captured face descriptor: 52\n",
      "Captured face descriptor: 53\n",
      "Captured face descriptor: 54\n",
      "Captured face descriptor: 55\n",
      "Captured face descriptor: 56\n",
      "Captured face descriptor: 57\n",
      "Captured face descriptor: 58\n",
      "Captured face descriptor: 59\n",
      "Captured face descriptor: 60\n",
      "Captured face descriptor: 61\n",
      "Captured face descriptor: 62\n",
      "Captured face descriptor: 63\n",
      "Captured face descriptor: 64\n",
      "Captured face descriptor: 65\n",
      "Captured face descriptor: 66\n",
      "Captured face descriptor: 67\n",
      "Captured face descriptor: 68\n",
      "Captured face descriptor: 69\n",
      "Captured face descriptor: 70\n",
      "Captured face descriptor: 71\n",
      "Captured face descriptor: 72\n",
      "Captured face descriptor: 73\n",
      "Captured face descriptor: 74\n",
      "Captured face descriptor: 75\n",
      "Captured face descriptor: 76\n",
      "Captured face descriptor: 77\n",
      "Captured face descriptor: 78\n",
      "Captured face descriptor: 79\n",
      "Captured face descriptor: 80\n",
      "Captured face descriptor: 81\n",
      "Captured face descriptor: 82\n",
      "Captured face descriptor: 83\n",
      "Captured face descriptor: 84\n",
      "Captured face descriptor: 85\n",
      "Captured face descriptor: 86\n",
      "Captured face descriptor: 87\n",
      "Captured face descriptor: 88\n",
      "Captured face descriptor: 89\n",
      "Captured face descriptor: 90\n",
      "Captured face descriptor: 91\n",
      "Captured face descriptor: 92\n",
      "Captured face descriptor: 93\n",
      "Captured face descriptor: 94\n",
      "Captured face descriptor: 95\n",
      "Captured face descriptor: 96\n",
      "Captured face descriptor: 97\n",
      "Captured face descriptor: 98\n",
      "Captured face descriptor: 99\n",
      "Captured face descriptor: 100\n",
      "Face descriptors saved.\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def generate_dataset():\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    sp = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "    facerec = dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')\n",
    "\n",
    "    def face_cropped(img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        dets = detector(gray, 1)\n",
    "        if len(dets) == 0:\n",
    "            return None\n",
    "        for d in dets:\n",
    "            shape = sp(gray, d)\n",
    "            face_descriptor = facerec.compute_face_descriptor(img, shape)\n",
    "            return np.array(face_descriptor)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    img_id = 0\n",
    "    face_descriptor_list = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture image\")\n",
    "            continue\n",
    "\n",
    "        face_descriptor = face_cropped(frame)\n",
    "        if face_descriptor is not None:\n",
    "            img_id += 1\n",
    "            face_descriptor_list.append(face_descriptor)\n",
    "            print(f\"Captured face descriptor: {img_id}\")\n",
    "\n",
    "        if cv2.waitKey(1) == 13 or img_id == 100:  # 13 is the ASCII character of Enter\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Save face descriptor\n",
    "    np.save('face_descriptor.npy', np.array(face_descriptor_list))\n",
    "    print(\"Face descriptors saved.\")\n",
    "\n",
    "generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a91ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def recognize_face():\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    sp = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "    facerec = dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')\n",
    "\n",
    "    known_face_descriptors = np.load('face_descriptor.npy')\n",
    "\n",
    "    def face_cropped(img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        dets = detector(gray, 1)\n",
    "        if len(dets) == 0:\n",
    "            return None, None\n",
    "        for d in dets:\n",
    "            shape = sp(gray, d)\n",
    "            face_descriptor = facerec.compute_face_descriptor(img, shape)\n",
    "            return np.array(face_descriptor), d\n",
    "        return None, None\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        face_descriptor, det = face_cropped(frame)\n",
    "        if face_descriptor is not None:\n",
    "            # Draw a box around the detected face\n",
    "            x, y, w, h = (det.left(), det.top(), det.right(), det.bottom())\n",
    "            cv2.rectangle(frame, (x, y), (w, h), (0, 255, 0), 2)\n",
    "            \n",
    "            # Compare with known face descriptors\n",
    "            distances = np.linalg.norm(known_face_descriptors - face_descriptor, axis=1)\n",
    "            min_distance = np.min(distances)\n",
    "            \n",
    "            if min_distance < 0.6:  # Adjust the threshold as needed\n",
    "                label = \"Kunal\"\n",
    "            else:\n",
    "                label = \"Unknown Person\"\n",
    "            \n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "recognize_face()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e867261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
